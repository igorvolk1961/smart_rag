"""
SGR Tool Calling Agent —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π streaming –∏ retry –ª–æ–≥–∏–∫–∏.
–ê–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω–æ –∏–∑ sgr-agent-core —Å –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ–º retry –ª–æ–≥–∏–∫–∏.
"""

from typing import Literal, Type

import openai
from openai import AsyncOpenAI, pydantic_function_tool
from loguru import logger

from api.agents.agent_definition import AgentConfig
from api.agents.base_agent import BaseAgent
from api.agents.models import AgentStatesEnum
from api.agents.base_tool import BaseTool
from api.agents.tools.final_answer_tool import FinalAnswerTool
from api.agents.tools.reasoning_tool import ReasoningTool


class SGRToolCallingAgent(BaseAgent):
    """Agent that uses OpenAI native function calling to select and execute
    tools based on SGR like a reasoning scheme."""

    name: str = "sgr_tool_calling_agent"

    def __init__(
        self,
        task_messages: list,
        openai_client: AsyncOpenAI,
        agent_config: AgentConfig,
        toolkit: list[Type[BaseTool]],
        def_name: str | None = None,
        **kwargs: dict,
    ):
        super().__init__(
            task_messages=task_messages,
            openai_client=openai_client,
            agent_config=agent_config,
            toolkit=toolkit,
            def_name=def_name,
            **kwargs,
        )
        self.tool_choice: Literal["required"] = "required"

    def _log_reasoning(self, result: ReasoningTool) -> None:
        """–õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ reasoning —Ñ–∞–∑—ã."""
        next_step = result.remaining_steps[0] if result.remaining_steps else "Completing"
        self.logger.info(
            f"""
    ###############################################
    ü§ñ LLM RESPONSE DEBUG:
       üß† Reasoning Steps: {result.reasoning_steps}
       üìä Current Situation: '{result.current_situation[:400]}...'
       üìã Plan Status: '{result.plan_status[:400]}...'
       üîç Searches Done: {self._context.searches_used}
       üîç Clarifications Done: {self._context.clarifications_used}
       ‚úÖ Enough Data: {result.enough_data}
       üìù Remaining Steps: {result.remaining_steps}
       üèÅ Task Completed: {result.task_completed}
       ‚û°Ô∏è Next Step: {next_step}
    ###############################################"""
        )
        self.log.append(
            {
                "step_number": self._context.iteration,
                "timestamp": self.creation_time.isoformat(),
                "step_type": "reasoning",
                "agent_reasoning": result.model_dump(mode="json"),
            }
        )

    async def _reasoning_phase(self) -> ReasoningTool:
        """Call LLM to decide next action based on current context with retry logic."""
        max_retries = self.config.execution.max_retries
        last_error: Exception | None = None

        for attempt in range(max_retries):
            try:
                self.logger.info(f"Reasoning phase (–ø–æ–ø—ã—Ç–∫–∞ {attempt + 1}/{max_retries})")
                
                async with self.openai_client.chat.completions.stream(
                    messages=await self._prepare_context(),
                    tools=[pydantic_function_tool(ReasoningTool, name=ReasoningTool.tool_name)],
                    tool_choice={"type": "function", "function": {"name": ReasoningTool.tool_name}},
                    **self.config.llm.to_openai_client_kwargs(),
                ) as stream:
                    async for event in stream:
                        if event.type == "chunk":
                            self.streaming_generator.add_chunk(event.chunk)
                    
                    completion = await stream.get_final_completion()
                    reasoning: ReasoningTool = (
                        completion.choices[0].message.tool_calls[0].function.parsed_arguments
                    )
                
                self.conversation.append(
                    {
                        "role": "assistant",
                        "content": None,
                        "tool_calls": [
                            {
                                "type": "function",
                                "id": f"{self._context.iteration}-reasoning",
                                "function": {
                                    "name": reasoning.tool_name,
                                    "arguments": reasoning.model_dump_json(),
                                },
                            }
                        ],
                    }
                )
                tool_call_result = await reasoning(self._context, self.config)
                self.streaming_generator.add_tool_call(
                    f"{self._context.iteration}-reasoning", reasoning.tool_name, tool_call_result
                )
                self.conversation.append(
                    {"role": "tool", "content": tool_call_result, "tool_call_id": f"{self._context.iteration}-reasoning"}
                )
                self._log_reasoning(reasoning)
                return reasoning

            except openai.AuthenticationError as e:
                # –û—à–∏–±–∫–∏ –∞—É—Ç–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏–∏ –Ω–µ –ø–æ–≤—Ç–æ—Ä—è–µ–º
                self.logger.error(f"–û—à–∏–±–∫–∞ –∞—É—Ç–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏–∏ LLM API: {e}")
                raise RuntimeError(f"–û—à–∏–±–∫–∞ –∞—É—Ç–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏–∏: {e}") from e
            except openai.RateLimitError as e:
                # –û—à–∏–±–∫–∏ –ª–∏–º–∏—Ç–∞ –Ω–µ –ø–æ–≤—Ç–æ—Ä—è–µ–º
                self.logger.error(f"–ü—Ä–µ–≤—ã—à–µ–Ω –ª–∏–º–∏—Ç –∑–∞–ø—Ä–æ—Å–æ–≤ LLM: {e}")
                raise RuntimeError(f"–ü—Ä–µ–≤—ã—à–µ–Ω –ª–∏–º–∏—Ç –∑–∞–ø—Ä–æ—Å–æ–≤: {e}") from e
            except openai.BadRequestError as e:
                # –û—à–∏–±–∫–∏ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –∑–∞–ø—Ä–æ—Å–∞ –Ω–µ –ø–æ–≤—Ç–æ—Ä—è–µ–º
                self.logger.error(f"–ù–µ–≤–µ—Ä–Ω—ã–π –∑–∞–ø—Ä–æ—Å –∫ LLM API: {e}")
                raise RuntimeError(f"–ù–µ–≤–µ—Ä–Ω—ã–π –∑–∞–ø—Ä–æ—Å: {e}") from e
            except (openai.APIConnectionError, openai.APITimeoutError, openai.OpenAIError) as e:
                # –û—à–∏–±–∫–∏ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è/—Ç–∞–π–º–∞—É—Ç–∞/–æ–±—â–∏–µ –æ—à–∏–±–∫–∏ LLM - –ø–æ–≤—Ç–æ—Ä—è–µ–º
                last_error = e
                if attempt < max_retries - 1:
                    self.logger.warning(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ reasoning (–ø–æ–ø—ã—Ç–∫–∞ {attempt + 1}/{max_retries}): {e}")
                    continue
                else:
                    self.logger.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å –≤—ã–ø–æ–ª–Ω–∏—Ç—å reasoning –ø–æ—Å–ª–µ {max_retries} –ø–æ–ø—ã—Ç–æ–∫: {e}")
                    raise RuntimeError(f"–û—à–∏–±–∫–∞ reasoning –ø–æ—Å–ª–µ {max_retries} –ø–æ–ø—ã—Ç–æ–∫: {e}") from e
            except Exception as e:
                # –î—Ä—É–≥–∏–µ –æ—à–∏–±–∫–∏ - –ø–æ–≤—Ç–æ—Ä—è–µ–º, –µ—Å–ª–∏ –µ—Å—Ç—å –ø–æ–ø—ã—Ç–∫–∏
                last_error = e
                if attempt < max_retries - 1:
                    self.logger.warning(f"–ù–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ reasoning (–ø–æ–ø—ã—Ç–∫–∞ {attempt + 1}/{max_retries}): {e}")
                    continue
                else:
                    self.logger.exception("–ù–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ reasoning")
                    raise RuntimeError(f"–û—à–∏–±–∫–∞ reasoning: {e}") from e

        # –ï—Å–ª–∏ –¥–æ—à–ª–∏ —Å—é–¥–∞, –∑–Ω–∞—á–∏—Ç –≤—Å–µ –ø–æ–ø—ã—Ç–∫–∏ –∏—Å—á–µ—Ä–ø–∞–Ω—ã
        if last_error:
            raise RuntimeError(f"–ù–µ —É–¥–∞–ª–æ—Å—å –≤—ã–ø–æ–ª–Ω–∏—Ç—å reasoning –ø–æ—Å–ª–µ {max_retries} –ø–æ–ø—ã—Ç–æ–∫: {last_error}") from last_error
        raise RuntimeError("–ù–µ —É–¥–∞–ª–æ—Å—å –≤—ã–ø–æ–ª–Ω–∏—Ç—å reasoning")

    async def _select_action_phase(self, reasoning: ReasoningTool) -> BaseTool:
        """Select the most suitable tool for the action decided in the reasoning phase with retry logic."""
        max_retries = self.config.execution.max_retries
        last_error: Exception | None = None

        for attempt in range(max_retries):
            try:
                self.logger.info(f"Select action phase (–ø–æ–ø—ã—Ç–∫–∞ {attempt + 1}/{max_retries})")
                
                async with self.openai_client.chat.completions.stream(
                    messages=await self._prepare_context(),
                    tools=await self._prepare_tools(),
                    tool_choice=self.tool_choice,
                    **self.config.llm.to_openai_client_kwargs(),
                ) as stream:
                    async for event in stream:
                        if event.type == "chunk":
                            self.streaming_generator.add_chunk(event.chunk)

                    completion = await stream.get_final_completion()

                # –ò–∑–≤–ª–µ–∫–∞–µ–º tool call –∏–∑ –æ—Ç–≤–µ—Ç–∞
                if not completion.choices or not completion.choices[0].message.tool_calls:
                    raise ValueError("LLM –Ω–µ –≤–µ—Ä–Ω—É–ª tool call. –ü—Ä–æ–≤–∞–π–¥–µ—Ä –º–æ–∂–µ—Ç –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—Ç—å tool calling –≤ streaming —Ä–µ–∂–∏–º–µ.")
                
                tool = completion.choices[0].message.tool_calls[0].function.parsed_arguments
                
                if not isinstance(tool, BaseTool):
                    raise ValueError("Selected tool is not a valid BaseTool instance")
                
                self.conversation.append(
                    {
                        "role": "assistant",
                        "content": reasoning.remaining_steps[0] if reasoning.remaining_steps else "Completing",
                        "tool_calls": [
                            {
                                "type": "function",
                                "id": f"{self._context.iteration}-action",
                                "function": {
                                    "name": tool.tool_name,
                                    "arguments": tool.model_dump_json(),
                                },
                            }
                        ],
                    }
                )
                self.streaming_generator.add_tool_call(
                    f"{self._context.iteration}-action", tool.tool_name, tool.model_dump_json()
                )
                return tool

            except openai.AuthenticationError as e:
                self.logger.error(f"–û—à–∏–±–∫–∞ –∞—É—Ç–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏–∏ LLM API: {e}")
                raise RuntimeError(f"–û—à–∏–±–∫–∞ –∞—É—Ç–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏–∏: {e}") from e
            except openai.RateLimitError as e:
                self.logger.error(f"–ü—Ä–µ–≤—ã—à–µ–Ω –ª–∏–º–∏—Ç –∑–∞–ø—Ä–æ—Å–æ–≤ LLM: {e}")
                raise RuntimeError(f"–ü—Ä–µ–≤—ã—à–µ–Ω –ª–∏–º–∏—Ç –∑–∞–ø—Ä–æ—Å–æ–≤: {e}") from e
            except openai.BadRequestError as e:
                self.logger.error(f"–ù–µ–≤–µ—Ä–Ω—ã–π –∑–∞–ø—Ä–æ—Å –∫ LLM API: {e}")
                raise RuntimeError(f"–ù–µ–≤–µ—Ä–Ω—ã–π –∑–∞–ø—Ä–æ—Å: {e}") from e
            except (openai.APIConnectionError, openai.APITimeoutError, openai.OpenAIError) as e:
                last_error = e
                if attempt < max_retries - 1:
                    self.logger.warning(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã–±–æ—Ä–µ –¥–µ–π—Å—Ç–≤–∏—è (–ø–æ–ø—ã—Ç–∫–∞ {attempt + 1}/{max_retries}): {e}")
                    continue
                else:
                    self.logger.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å –≤—ã–±—Ä–∞—Ç—å –¥–µ–π—Å—Ç–≤–∏–µ –ø–æ—Å–ª–µ {max_retries} –ø–æ–ø—ã—Ç–æ–∫: {e}")
                    raise RuntimeError(f"–û—à–∏–±–∫–∞ –≤—ã–±–æ—Ä–∞ –¥–µ–π—Å—Ç–≤–∏—è –ø–æ—Å–ª–µ {max_retries} –ø–æ–ø—ã—Ç–æ–∫: {e}") from e
            except Exception as e:
                last_error = e
                if attempt < max_retries - 1:
                    self.logger.warning(f"–ù–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã–±–æ—Ä–µ –¥–µ–π—Å—Ç–≤–∏—è (–ø–æ–ø—ã—Ç–∫–∞ {attempt + 1}/{max_retries}): {e}")
                    continue
                else:
                    self.logger.exception("–ù–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã–±–æ—Ä–µ –¥–µ–π—Å—Ç–≤–∏—è")
                    raise RuntimeError(f"–û—à–∏–±–∫–∞ –≤—ã–±–æ—Ä–∞ –¥–µ–π—Å—Ç–≤–∏—è: {e}") from e

        if last_error:
            raise RuntimeError(f"–ù–µ —É–¥–∞–ª–æ—Å—å –≤—ã–±—Ä–∞—Ç—å –¥–µ–π—Å—Ç–≤–∏–µ –ø–æ—Å–ª–µ {max_retries} –ø–æ–ø—ã—Ç–æ–∫: {last_error}") from last_error
        raise RuntimeError("–ù–µ —É–¥–∞–ª–æ—Å—å –≤—ã–±—Ä–∞—Ç—å –¥–µ–π—Å—Ç–≤–∏–µ")

    async def _action_phase(self, tool: BaseTool) -> str:
        """Call Tool for the action decided in the select_action phase."""
        result = await tool(self._context, self.config)
        self.conversation.append(
            {"role": "tool", "content": result, "tool_call_id": f"{self._context.iteration}-action"}
        )
        self.streaming_generator.add_chunk_from_str(f"{result}\n")
        self._log_tool_execution(tool, result)
        return result
