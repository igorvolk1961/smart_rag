"""
–ë–∞–∑–æ–≤—ã–π –∫–ª–∞—Å—Å –¥–ª—è –∞–≥–µ–Ω—Ç–æ–≤.
–ê–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω–æ –∏–∑ sgr-agent-core.
"""

import json
import logging
import os
import traceback
import uuid
from datetime import datetime
from typing import Type

from openai import AsyncOpenAI, pydantic_function_tool
from openai.types.chat import ChatCompletionFunctionToolParam, ChatCompletionMessageParam

from api.agents.agent_definition import AgentConfig
from api.agents.models import AgentContext, AgentStatesEnum
from api.agents.services.prompt_loader import PromptLoader
from api.agents.registry import AgentRegistry
from api.agents.stream import OpenAIStreamingGenerator
from api.agents.base_tool import BaseTool
from loguru import logger


class AgentRegistryMixin:
    def __init_subclass__(cls, **kwargs):
        super().__init_subclass__(**kwargs)
        if cls.__name__ not in ("BaseAgent",):
            AgentRegistry.register(cls, name=cls.name)


class BaseAgent(AgentRegistryMixin):
    """Base class for agents."""

    name: str = "base_agent"

    def __init__(
        self,
        task_messages: list[ChatCompletionMessageParam],
        openai_client: AsyncOpenAI,
        agent_config: AgentConfig,
        toolkit: list[Type[BaseTool]],
        def_name: str | None = None,
        **kwargs: dict,
    ):
        self.id = f"{def_name or self.name}_{uuid.uuid4()}"
        self.openai_client = openai_client
        self.config = agent_config
        self.creation_time = datetime.now()
        self.task_messages = task_messages
        self.toolkit = toolkit

        self._context = AgentContext()
        self.conversation = []

        self.streaming_generator = OpenAIStreamingGenerator(model=self.config.llm.model or "gpt-4o")
        self.logger = logger.bind(agent_id=self.id)
        self.log = []

    async def provide_clarification(self, messages: list[ChatCompletionMessageParam]):
        """Receive clarification from an external source (e.g. user input) in
        OpenAI messages format."""
        self.conversation.extend(messages)
        self.conversation.append(
            {"role": "user", "content": PromptLoader.get_clarification_template(messages, self.config.prompts)}
        )

        self._context.clarifications_used += 1
        self._context.clarification_received.set()
        self._context.state = AgentStatesEnum.RESEARCHING
        self.logger.info(f"‚úÖ Clarification received: {len(messages)} messages")

    def _log_reasoning(self, result) -> None:
        """–õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ reasoning —Ñ–∞–∑—ã."""
        # –ë—É–¥–µ—Ç –ø–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–æ –≤ –ø–æ–¥–∫–ª–∞—Å—Å–∞—Ö —Å –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–º —Ç–∏–ø–æ–º ReasoningTool
        self.logger.debug(f"Reasoning phase completed: {result}")

    def _log_tool_execution(self, tool: BaseTool, result: str):
        """–õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è tool."""
        self.logger.info(
            f"""
###############################################
üõ†Ô∏è TOOL EXECUTION DEBUG:
    üîß Tool Name: {tool.tool_name}
    üìã Tool Model: {tool.model_dump_json(indent=2)}
    üîç Result: '{result[:400]}...'
###############################################"""
        )
        self.log.append(
            {
                "step_number": self._context.iteration,
                "timestamp": datetime.now().isoformat(),
                "step_type": "tool_execution",
                "tool_name": tool.tool_name,
                "agent_tool_context": tool.model_dump(mode="json"),
                "agent_tool_execution_result": result,
            }
        )

    def _save_agent_log(self):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ª–æ–≥–æ–≤ –∞–≥–µ–Ω—Ç–∞."""
        logs_dir = self.config.execution.logs_dir
        # Skip saving if logs_dir is None or empty string
        if not logs_dir:
            self.logger.debug("Skipping agent log save: logs_dir is not configured")
            return

        os.makedirs(logs_dir, exist_ok=True)
        filepath = os.path.join(logs_dir, f"{datetime.now().strftime('%Y%m%d-%H%M%S')}-{self.id}-log.json")
        agent_log = {
            "id": self.id,
            "model_config": self.config.llm.model_dump(
                exclude={"api_key", "proxy"}, mode="json"
            ),  # Sensitive data excluded by default
            "task_messages": self.task_messages,
            "toolkit": [tool.tool_name for tool in self.toolkit],
            "log": self.log,
        }

        json.dump(agent_log, open(filepath, "w", encoding="utf-8"), indent=2, ensure_ascii=False)

    async def _prepare_context(self) -> list[dict]:
        """Prepare a conversation context with system prompt, task data and any
        other context.

        Note: Override this method to change the context setup for the agent.

        Returns a list of dictionaries OpenAI like format, each
        containing a role and content key by default.
        """

        # –§–æ—Ä–º–∏—Ä—É–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç: —Å–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç + task_messages + conversation
        context_messages = [
            {"role": "system", "content": PromptLoader.get_system_prompt(self.toolkit, self.config.prompts)},
        ]
        
        # –î–æ–±–∞–≤–ª—è–µ–º task_messages (–æ–Ω–∏ —É–∂–µ —Å–æ–¥–µ—Ä–∂–∞—Ç –∑–∞–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è)
        context_messages.extend(self.task_messages)
        
        # –î–æ–±–∞–≤–ª—è–µ–º conversation (–∏—Å—Ç–æ—Ä–∏—è –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è –∞–≥–µ–Ω—Ç–∞ —Å tools)
        context_messages.extend(self.conversation)
        
        return context_messages

    async def _prepare_tools(self) -> list[ChatCompletionFunctionToolParam]:
        """Prepare available tools for the current agent state and progress.

        Note: Override this method to change the tool setup or conditions for tool
        usage.

        Returns a list of ChatCompletionFunctionToolParam based
        available tools.
        """
        tools = set(self.toolkit)
        if self._context.iteration >= self.config.execution.max_iterations:
            raise RuntimeError("Max iterations reached")
        return [pydantic_function_tool(tool, name=tool.tool_name) for tool in tools]

    async def _reasoning_phase(self):
        """Call LLM to decide next action based on current context."""
        raise NotImplementedError("_reasoning_phase must be implemented by subclass")

    async def _select_action_phase(self, reasoning):
        """Select the most suitable tool for the action decided in the
        reasoning phase.

        Returns the tool suitable for the action.
        """
        raise NotImplementedError("_select_action_phase must be implemented by subclass")

    async def _action_phase(self, tool: BaseTool) -> str:
        """Call Tool for the action decided in the select_action phase.

        Returns string or dumped JSON result of the tool execution.
        """
        raise NotImplementedError("_action_phase must be implemented by subclass")

    async def _execution_step(self):
        """Execute a single step of the agent workflow.

        Note: Override this method to change the agent workflow for each step.
        """
        reasoning = await self._reasoning_phase()
        self._context.current_step_reasoning = reasoning
        action_tool = await self._select_action_phase(reasoning)
        await self._action_phase(action_tool)

        # ClarificationTool –±—É–¥–µ—Ç –æ–±—Ä–∞–±–æ—Ç–∞–Ω –≤ –ø–æ–¥–∫–ª–∞—Å—Å–∞—Ö –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏
        # if isinstance(action_tool, ClarificationTool):
        #     self.logger.info("\n‚è∏Ô∏è  Research paused - please answer questions")
        #     self._context.state = AgentStatesEnum.WAITING_FOR_CLARIFICATION
        #     self.streaming_generator.finish()
        #     self._context.clarification_received.clear()
        #     await self._context.clarification_received.wait()

    async def execute(
        self,
    ):
        self.logger.info(f"üöÄ User provided {len(self.task_messages)} messages.")
        try:
            while self._context.state not in AgentStatesEnum.FINISH_STATES.value:
                self._context.iteration += 1
                self.logger.info(f"Step {self._context.iteration} started")
                await self._execution_step()
            return self._context.execution_result

        except Exception as e:
            self.logger.error(f"‚ùå Agent execution error: {str(e)}")
            self._context.state = AgentStatesEnum.FAILED
            traceback.print_exc()
        finally:
            if self.streaming_generator is not None:
                self.streaming_generator.finish(self._context.execution_result)
            self._save_agent_log()
